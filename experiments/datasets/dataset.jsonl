{"qid":"bert_q1","question":"According to the 'BERT' paper, what are the two unsupervised pre-training tasks?","gold_answer":"The two pre-training tasks are Masked LM (MLM) and Next Sentence Prediction (NSP).","gold_citations":["bert.pdf"]}
{"qid":"bert_q2","question":"What are the architectural specifications (Layers, Hidden size, Attention heads) of the BERT-BASE model?","gold_answer":"The BERT-BASE model has 12 layers, a hidden size of 768, and 12 self-attention heads.","gold_citations":["bert.pdf"]}
{"qid":"bert_q3","question":"In the BERT pre-training data, what two corpora were used?","gold_answer":"The pre-training corpus used the BooksCorpus (800M words) and English Wikipedia (2,500M words).","gold_citations":["bert.pdf"]}
{"qid":"bert_q4","question":"During BERT's MLM pre-training, what percentage of tokens are masked at random?","gold_answer":"In the masked LM (MLM) pre-training, 15% of all WordPiece tokens in each sequence are masked at random.","gold_citations":["bert.pdf"]}
{"qid":"attn_q1","question":"In 'Attention Is All You Need', why is the dot-product attention scaled by 1/sqrt(d_k)?","gold_answer":"To counteract the effect of large dot products pushing the softmax function into regions with extremely small gradients.","gold_citations":["attention_is_all_you_need.pdf"]}
{"qid":"attn_q2","question":"How many layers are in the encoder and decoder stacks of the base Transformer model in 'Attention is All You Need'?","gold_answer":"The encoder and decoder are each composed of a stack of N=6 identical layers.","gold_citations":["attention_is_all_you_need.pdf"]}
{"qid":"attn_q3","question":"How many parallel attention heads are used in the base Transformer model described in 'Attention is All You Need'?","gold_answer":"The base model employs h=8 parallel attention layers, or heads.","gold_citations":["attention_is_all_you_need.pdf"]}
{"qid":"attn_q4","question":"What optimizer was used to train the Transformer models in 'Attention Is All You Need'?","gold_answer":"The Adam optimizer was used with specific beta values (β1=0.9, β2=0.98) and epsilon (ε=10^-9).","gold_citations":["attention_is_all_you_need.pdf"]}
{"qid":"best_q1","question":"According to 'Searching for Best Practices in RAG', what is the main trade-off of using the 'Hybrid with HyDE' retrieval method?","gold_answer":"The 'Hybrid with HyDE' method attained the highest RAG score but at a considerable computational cost, with a latency of 11.71 seconds per query.","gold_citations":["bestpractices.pdf"]}
{"qid":"best_q2","question":"In the 'Searching for Best Practices in RAG' paper, which repacking configuration showed superior performance?","gold_answer":"The 'Reverse' repacking configuration exhibited superior performance.","gold_citations":["bestpractices.pdf"]}
{"qid":"docagent_q1","question":"What is the primary function of the 'Navigator' module in the DocAgent system?","gold_answer":"The Navigator module's primary function is to establish a dependency-aware processing order by performing a topological sort on a dependency graph of the codebase.","gold_citations":["docagent.pdf"]}
{"qid":"docagent_q2","question":"What are the three dimensions of the evaluation framework proposed in the DocAgent paper?","gold_answer":"The three evaluation dimensions are Completeness, Helpfulness, and Truthfulness.","gold_citations":["docagent.pdf"]}
{"qid":"deepseekv3_q1","question":"According to the DeepSeek-V3 technical report, what is Multi-Token Prediction (MTP)?","gold_answer":"Multi-Token Prediction (MTP) is a training objective that extends the prediction scope to multiple future tokens at each position, rather than just the next single token.","gold_citations":["deepseekv3.pdf"]}
{"qid":"deepseekv3_q2","question":"On how many tokens was the DeepSeek-V3 model pre-trained?","gold_answer":"The DeepSeek-V3 model was pre-trained on 14.8 trillion high-quality and diverse tokens.","gold_citations":["deepseekv3.pdf"]}
{"qid":"deepseekr1_q1","question":"What reinforcement learning algorithm was used to train the DeepSeek-R1 series of models?","gold_answer":"Group Relative Policy Optimization (GRPO) was used as the reinforcement learning algorithm.","gold_citations":["deepseekr1.pdf"]}
{"qid":"no_rag_q1","question":"What is the chemical formula for water?","gold_answer":"The chemical formula for water is H2O."}
{"qid":"no_rag_q2","question":"Briefly explain the process of photosynthesis.","gold_answer":"Photosynthesis is the process used by plants, algae, and certain bacteria to convert light energy into chemical energy, through a process that transforms carbon dioxide and water into glucose (sugar) and oxygen."}
{"qid":"no_rag_q3","question":"What is 15 multiplied by 12?","gold_answer":"15 multiplied by 12 is 180."}
{"qid":"no_rag_q4","question":"Write a short, four-line poem about the ocean.","gold_answer":"Waves that crash upon the shore,\nA constant, deep, and rhythmic roar,\nA world of blue, so vast and wide,\nWhere secrets of the deep reside."}
{"qid":"no_rag_q5","question":"Who was the first person to walk on the moon?","gold_answer":"The first person to walk on the moon was Neil Armstrong."}
